{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/ataleckij/Projects/university/mo/Data/Lab 4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ex4data1.mat', 'ex4weights.mat']\n"
     ]
    }
   ],
   "source": [
    "files_names = os.listdir(DATA_PATH)\n",
    "print(files_names)\n",
    "files_paths = [os.path.join(DATA_PATH, name) for name in files_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 400) (5000,)\n",
      "(25, 401) (10, 26)\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "\n",
    "data = loadmat(files_paths[0])\n",
    "weights = loadmat(files_paths[1])\n",
    "\n",
    "features, labels = data['X'], data['y'].flatten()\n",
    "t1, t2 = weights['Theta1'], weights['Theta2']\n",
    "print(features.shape, labels.shape)\n",
    "print(t1.shape, t2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 10 10 10 10 10 10]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels_indexes = le.fit_transform(labels)\n",
    "\n",
    "labels_onehot = np.zeros((labels.shape[0], labels_indexes.max() + 1))\n",
    "labels_onehot[np.arange(labels.shape[0]), labels_indexes] = 1\n",
    "\n",
    "print(labels[:7])\n",
    "print(labels_onehot[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1 = t1[:, 1:], t1[:, [0]]\n",
    "w2, b2 = t2[:, 1:], t2[:, [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "# def cost_function(network, test_data):\n",
    "#     features, labels_onehot = test_data\n",
    "    \n",
    "#     c = 0\n",
    "#     for example, y_onehot in test_data:\n",
    "#         prediction_onehot = network.feedforward(example)\n",
    "#         c += np.sum((y_onehot - prediction_onehot)**2)\n",
    "#     return c / len(test_data)\n",
    "\n",
    "def cost_function(prediction_onehot, y_onehot, w, lambda_):\n",
    "    return (np.sum(y_onehot, -prediction_onehot, axis=1) ** 2).mean() + 1/2*lambda_ * w.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, sizes, output=True):\n",
    "        \"\"\"\n",
    "        Список ``sizes`` содержит количество нейронов в соответствующих слоях\n",
    "        нейронной сети. Смещения и веса для нейронных сетей\n",
    "        инициализируются случайными значениями, подчиняющимися стандартному нормальному\n",
    "        распределению. Первый слой подразумевается слоем, принимающим входные данные, \n",
    "        поэтому к нему не добавляется смещение\n",
    "        \"\"\"\n",
    "\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        self.output = output\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        Вычислить и вернуть выходную активацию нейронной сети\n",
    "        при получении ``a`` на входе\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b.T[0])\n",
    "        return a\n",
    "    \n",
    "    def predict(self, features):\n",
    "        return np.asarray([self.feedforward(example_features) \n",
    "                           for example_features in features])\n",
    "    \n",
    "    def predict_classes(self, features):\n",
    "        prediction_onehot = self.predict(features)\n",
    "        return np.argmax(prediction_onehot, axis=1)\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"\n",
    "        Обучить нейронную сеть, используя алгоритм стохастического\n",
    "        (mini-batch) градиентного спуска. \n",
    "        ``training_data`` - лист кортежей вида ``(x, y)``, где \n",
    "        x - вход обучающего примера, y - желаемый выход (в формате one-hot). \n",
    "        Роль остальных обязательных параметров должна быть понятна из их названия.\n",
    "        Если предоставлен опциональный аргумент ``test_data``, \n",
    "        то после каждой эпохи обучения сеть будет протестирована на этих данных \n",
    "        и промежуточный результат обучения будет выведен в консоль. \n",
    "        ``test_data`` -- это список кортежей из входных данных \n",
    "        и номеров правильных классов примеров (т.е. argmax(y),\n",
    "        если y -- набор ответов в той же форме, что и в тренировочных данных).\n",
    "        Тестирование полезно для мониторинга процесса обучения,\n",
    "        но может существенно замедлить работу программы.\n",
    "        \"\"\"\n",
    "\n",
    "        if test_data is not None: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        success_tests = 0\n",
    "        training_data = [(x[:, np.newaxis], y[:, np.newaxis]) for x, y in training_data]\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size]\n",
    "                            for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data is not None and self.output:\n",
    "                success_tests = self.evaluate(test_data)\n",
    "                print(\"Эпоха {0}: {1} / {2}\".format(j, success_tests, n_test))\n",
    "            elif self.output:\n",
    "                print(\"Эпоха {0} завершена\".format(j))\n",
    "        if test_data is not None:\n",
    "            return success_tests / n_test\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Обновить веса и смещения нейронной сети, сделав шаг градиентного\n",
    "        спуска на основе алгоритма обратного распространения ошибки, примененного\n",
    "        к одному mini batch.\n",
    "        ``mini_batch`` - список кортежей вида ``(x, y)``,\n",
    "        ``eta`` - величина шага (learning rate).\n",
    "        \"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        eps = eta / len(mini_batch)\n",
    "        self.weights = [w - eps * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases  = [b - eps * nb for b, nb in zip(self.biases,  nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "        Возвращает кортеж ``(nabla_b, nabla_w)`` -- градиент целевой функции по всем параметрам сети.\n",
    "        ``nabla_b`` и ``nabla_w`` -- послойные списки массивов ndarray,\n",
    "        такие же, как self.biases и self.weights соответственно.\n",
    "        \"\"\"\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "\n",
    "        # прямое распространение (forward pass)\n",
    "\n",
    "        a = [np.array(x).copy()]\n",
    "        z = []\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            # посчитать активации\n",
    "            if len(a) > 1:\n",
    "                zi = np.dot(w, a[-1]) + b#.T[0]\n",
    "            else:\n",
    "                zi = np.dot(w, a[-1])# + b#.T[0]\n",
    "            ai = sigmoid(zi)\n",
    "            z.append(zi)\n",
    "            a.append(ai)\n",
    "        z = np.array(z)\n",
    "        a = np.array(a)\n",
    "\n",
    "        # обратное распространение (backward pass)\n",
    "        delta = (a[-1] - y)*sigmoid_prime(z[-1])#/(a[-1]*(1-a[-1]))  # ошибка выходного слоя\n",
    "        nabla_b[-1] = delta # производная J по смещениям выходного слоя\n",
    "        nabla_w[-1] = np.dot(delta, a[-2].T)  # производная J по весам выходного слоя\n",
    "\n",
    "        for l in range(2, self.num_layers):\n",
    "            # дополнительные вычисления, чтобы легче записывалось\n",
    "            #\n",
    "            weights = np.array(self.weights).copy()\n",
    "            delta = np.dot(weights[-(l-1)].T, delta)*sigmoid_prime(z[-l]) # ошибка на слое L-l\n",
    "            nabla_b[-l] = delta # производная J по смещениям L-l-го слоя\n",
    "            nabla_w[-l] = np.dot(delta, a[-(l+1)].T) # производная J по весам L-l-го слоя\n",
    "        return nabla_b, nabla_w\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"\n",
    "        Вернуть количество тестовых примеров, для которых нейронная сеть\n",
    "        возвращает правильный ответ.\n",
    "        \"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"\n",
    "        Возвращает вектор частных производных целевой функции по активациям выходного слоя.\n",
    "        \"\"\"\n",
    "        return (output_activations-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(25, 1), (10, 1)]\n",
      "[(25, 400), (10, 25)]\n",
      "(25, 1) (10, 1)\n",
      "(25, 400) (10, 25)\n"
     ]
    }
   ],
   "source": [
    "nn = Network([400, 25, 10])\n",
    "\n",
    "print([b.shape for b in nn.biases])\n",
    "print([w.shape for w in nn.weights])\n",
    "\n",
    "print(b1.shape, b2.shape)\n",
    "print(w1.shape, w2.shape)\n",
    "\n",
    "nn.biases = [b1, b2]\n",
    "nn.weights = [w1, w2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9752"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indexes = nn.predict_classes(features)\n",
    "predicted_labels = le.inverse_transform(predicted_indexes)\n",
    "\n",
    "(predicted_labels == labels).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedNetwork(Network):\n",
    "    def __init__(self, sizes, output=True, l2=0):\n",
    "        super().__init__(sizes, output)\n",
    "        self.l2 = l2\n",
    "        \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        Обновить веса и смещения нейронной сети, сделав шаг градиентного\n",
    "        спуска на основе алгоритма обратного распространения ошибки, примененного\n",
    "        к одному mini batch. Учесть штраф L2.\n",
    "        ``mini_batch`` - список кортежей вида ``(x, y)``,\n",
    "        ``eta`` - величина шага (learning rate).\n",
    "        \"\"\"\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        eps = eta / len(mini_batch)\n",
    "        self.weights = [w - eps * nw - self.l2 * w for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases  = [b - eps * nb for b, nb in zip(self.biases,  nabla_b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 0 завершена\n",
      "Эпоха 1 завершена\n",
      "Эпоха 2 завершена\n",
      "Эпоха 3 завершена\n",
      "Эпоха 4 завершена\n",
      "Эпоха 5 завершена\n",
      "Эпоха 6 завершена\n",
      "Эпоха 7 завершена\n",
      "Эпоха 8 завершена\n",
      "Эпоха 9 завершена\n",
      "Эпоха 10 завершена\n",
      "Эпоха 11 завершена\n",
      "Эпоха 12 завершена\n",
      "Эпоха 13 завершена\n",
      "Эпоха 14 завершена\n",
      "Эпоха 15 завершена\n",
      "Эпоха 16 завершена\n",
      "Эпоха 17 завершена\n",
      "Эпоха 18 завершена\n",
      "Эпоха 19 завершена\n"
     ]
    }
   ],
   "source": [
    "nn_l2 = RegularizedNetwork([400, 25, 10], l2=0.00001)\n",
    "\n",
    "train_data = list(zip(features, labels_onehot))\n",
    "nn_l2.SGD(train_data, eta=0.5, mini_batch_size=16, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8146"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_indexes = nn_l2.predict_classes(features)\n",
    "predicted_labels = le.inverse_transform(predicted_indexes)\n",
    "\n",
    "(predicted_labels == labels).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
